{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project 3: Reddit NLP\n",
    "#### Corey J Sinnott\n",
    "# Data Collection\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report was commissioned to perform natural language processing (NLP) and analysis on two subreddits of Reddit.com. Data includes over 8000 posts, 4000 belonging to r/AskALiberal, and 4000 belonging to r/AskAConservative. The problem statement was defined as, can we classify to which subreddit a post belongs? After in-depth analysis, conclusions and recommendations will be presented.\n",
    "\n",
    "*See model_classification_exec_summary.ipynb for the full summary, data dictionary, and findings.*\n",
    "\n",
    "## Contents:\n",
    "- [API Testing](#API-Testing)\n",
    "- [Defining a Function](#Defining-a-Function)\n",
    "- [Data Collection](#Data-Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Testing \n",
    " - Testing the basic functions of the Pushshift API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://api.pushshift.io/reddit/search/submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'subreddit' : 'askaliberal',\n",
    "    'size' : 25,\n",
    "    #'before': last_post\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial test of url successful\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_awardings': [],\n",
       " 'allow_live_comments': False,\n",
       " 'author': 'EsperantistoUsona',\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_template_id': '69751e82-c00e-11e7-bf56-0e79ff121398',\n",
       " 'author_flair_text': 'Social Democrat',\n",
       " 'author_flair_text_color': 'dark',\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_7u4gzwvn',\n",
       " 'author_patreon_flair': False,\n",
       " 'author_premium': False,\n",
       " 'awarders': [],\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1610947103,\n",
       " 'domain': 'self.AskALiberal',\n",
       " 'full_link': 'https://www.reddit.com/r/AskALiberal/comments/kznxgi/whats_a_good_political_thriller_that_talks_about/',\n",
       " 'gildings': {},\n",
       " 'id': 'kznxgi',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': True,\n",
       " 'num_comments': 1,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/AskALiberal/comments/kznxgi/whats_a_good_political_thriller_that_talks_about/',\n",
       " 'pinned': False,\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1610947114,\n",
       " 'score': 1,\n",
       " 'selftext': \"Bit of a fun post. I figured it would be nice to not have one about the shitstorm coming on the 20th, so yeah, what's a good political thriller?\",\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'AskALiberal',\n",
       " 'subreddit_id': 't5_2ukxe',\n",
       " 'subreddit_subscribers': 21523,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'self',\n",
       " 'title': \"What's a good political thriller that talks about how congress actually works? I.e it follows a congress person trying to get a bill passed and the deals and favors they call in. Are you aware of any such thrillers?\",\n",
       " 'total_awards_received': 0,\n",
       " 'treatment_tags': [],\n",
       " 'upvote_ratio': 1.0,\n",
       " 'url': 'https://www.reddit.com/r/AskALiberal/comments/kznxgi/whats_a_good_political_thriller_that_talks_about/',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test successful\n",
    "data = res.json()['data']\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lib_df = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])\n",
    "full_cons_df = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def full_pull(subreddit_1, subreddit_2, size, iterations):\n",
    "#     \"\"\"\n",
    "#     Pulls posts by a specified amount, with time breaks between iterations.\n",
    "\n",
    "#     Args:\n",
    "#         subreddit_1 (string): name of first subreddit\n",
    "#         subreddit_2 (string): name of first subreddit\n",
    "#         size (int)          : number of posts to be pulled per iteration\n",
    "#         iterations (int)    : the number of pulls to be performed\n",
    "        \n",
    "#     Returns:\n",
    "#         full_post_df (pandas DataFrame): dataframe containing the complete, \n",
    "#         raw collection of pulled posta\n",
    "#     \"\"\"\n",
    "#     utc = 1610983983\n",
    "#     full_lib_df  = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])\n",
    "#     full_cons_df = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])\n",
    "    \n",
    "#     for pull in range(iterations):\n",
    "#         url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit_1}&size={size}&before={utc}'\n",
    "#         res = requests.get(url)\n",
    "#         data = res.json()['data']\n",
    "#         pull_dict_1 = {\n",
    "#             'title'      : [],\n",
    "#             'selftext'   : [],\n",
    "#             'subreddit'  : [],\n",
    "#             'created_utc': []\n",
    "#                             }\n",
    "#         #if res.status_code == 200: #optional; helpful for troubleshooting\n",
    "#         for i in data:\n",
    "#             pull_dict_1['title'].append(i['title'])\n",
    "#             pull_dict_1['selftext'].append(i['selftext'])\n",
    "#             pull_dict_1['subreddit'].append(i['subreddit'])\n",
    "#             pull_dict_1['created_utc'].append(i['created_utc'])\n",
    "#             #print(f'You have obtained {len(pull_dict)} posts') #real-time counter\n",
    "#         temp_posts_1 = pd.DataFrame(pull_dict_1)\n",
    "#         full_lib_df = pd.concat([full_lib_df, temp_posts_1])\n",
    "#         utc = full_lib_df['created_utc'].astype('int64').min() #pulls the final timestamp\n",
    "#         time.sleep(30)                                            #to obtain unique data\n",
    "        \n",
    "#     print(f'Pull complete; you have obtained {len(full_lib_df)} total posts from r/{subreddit_1}')\n",
    "    \n",
    "# #-------------- second subreddit ---------------------------------------------------------------------------#        \n",
    "#     time.sleep(30)\n",
    "#     for pull_2 in range(iterations):\n",
    "#         url_2 = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit_2}&size={size}&before={utc}'\n",
    "#         res_2 = requests.get(url_2)\n",
    "#         data_2 = res_2.json()['data']\n",
    "#         pull_dict_2 = {\n",
    "#             'title'      : [],\n",
    "#             'selftext'   : [],\n",
    "#             'subreddit'  : [],\n",
    "#             'created_utc': []\n",
    "#                             }\n",
    "#         #if res.status_code == 200: #optional; helpful for troubleshooting\n",
    "#         for i in data_2:\n",
    "#             pull_dict_2['title'].append(i['title'])\n",
    "#             pull_dict_2['selftext'].append(i['selftext'])\n",
    "#             pull_dict_2['subreddit'].append(i['subreddit'])\n",
    "#             pull_dict_2['created_utc'].append(i['created_utc'])\n",
    "#             #print(f'You have obtained {len(pull_dict)} posts') #real-time counter\n",
    "#         temp_posts_2 = pd.DataFrame(pull_dict_2)\n",
    "#         full_cons_df = pd.concat([full_cons_df, temp_posts_2])\n",
    "#         utc = full_cons_df['created_utc'].astype('int64').min() #pulls the final timestamp\n",
    "#         time.sleep(30)                                            #to obtain unique data\n",
    "    \n",
    "#     print(f'Pull complete; you have obtained {len(full_cons_df)} total posts from r/{subreddit_2}')\n",
    "        \n",
    "#         #else:\n",
    "#             #print(res.status_code)                    \n",
    "# #---------- combine dfs -----------------------\n",
    "    \n",
    "#     full_pull_df = pd.concat([full_lib_df, full_cons_df])\n",
    "    \n",
    "#     return full_pull_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Function\n",
    " - Function's purpose is to utilize the Pushshift API while overcoming its size limitation.\n",
    " - Sleep timers were used to overcome rate limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pull(subreddit_1, subreddit_2, size, iterations):\n",
    "    \"\"\"\n",
    "    Pulls posts by a specified amount, with time breaks between iterations.\n",
    "\n",
    "    Args:\n",
    "        subreddit_1 (string): name of first subreddit\n",
    "        subreddit_2 (string): name of first subreddit\n",
    "        size (int)          : number of posts to be pulled per iteration\n",
    "        iterations (int)    : the number of pulls to be performed\n",
    "        \n",
    "    Returns:\n",
    "        full_post_df (pandas DataFrame): dataframe containing the complete, \n",
    "        raw collection of pulled posta\n",
    "    \"\"\"\n",
    "    utc = 1610946272\n",
    "    full_lib_df = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])\n",
    "    full_cons_df = pd.DataFrame(columns = ['title', 'selftext', 'subreddit', 'created_utc'])\n",
    "    \n",
    "    for pull in range(iterations):\n",
    "        url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit_1}&size={size}&before={utc}'\n",
    "        res = requests.get(url)\n",
    "        data = res.json()['data']\n",
    "        pull_dict_1 = {\n",
    "            'title'      : [],\n",
    "            'selftext'   : [],\n",
    "            'subreddit'  : [],\n",
    "            'created_utc': []\n",
    "                            }\n",
    "        #if res.status_code == 200: #optional; helpful for troubleshooting\n",
    "        for i in data:\n",
    "            try:\n",
    "                pull_dict_1['title'].append(i['title'])\n",
    "                pull_dict_1['subreddit'].append(i['subreddit'])\n",
    "                pull_dict_1['created_utc'].append(i['created_utc'])\n",
    "                \n",
    "                try: # some posts are missing 'self-text'\n",
    "                    pull_dict_1['selftext'].append(i['selftext'])\n",
    "                except:\n",
    "                    pull_dict_1['selftext'].append(['not there'])\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #print(f'You have obtained {len(pull_dict_1)} posts') #real-time counter\n",
    "        temp_posts_1 = pd.DataFrame(pull_dict_1)\n",
    "        full_lib_df = pd.concat([full_lib_df, temp_posts_1])\n",
    "        utc = full_lib_df['created_utc'].astype('int64').min() #pulls the final timestamp\n",
    "        time.sleep(30)                                            #to obtain unique data\n",
    "        \n",
    "    print(f'Pull complete; you have obtained {len(full_lib_df)} total posts from r/{subreddit_1}')\n",
    "    \n",
    "#-------------- second subreddit ---------------------------------------------------------------------------#        \n",
    "    time.sleep(30)\n",
    "    for pull_2 in range(iterations):\n",
    "        url_2 = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit_2}&size={size}&before={utc}'\n",
    "        res_2 = requests.get(url_2)\n",
    "        data_2 = res_2.json()['data']\n",
    "        pull_dict_2 = {\n",
    "            'title'      : [],\n",
    "            'selftext'   : [],\n",
    "            'subreddit'  : [],\n",
    "            'created_utc': []\n",
    "                            }\n",
    "        #if res.status_code == 200: #optional; helpful for troubleshooting\n",
    "        for i in data_2:\n",
    "            try:\n",
    "                pull_dict_2['title'].append(i['title'])\n",
    "                pull_dict_2['subreddit'].append(i['subreddit'])\n",
    "                pull_dict_2['created_utc'].append(i['created_utc'])\n",
    "                \n",
    "                try: # some posts are missing 'self-text'\n",
    "                    pull_dict_2['selftext'].append(i['selftext'])\n",
    "                except:\n",
    "                    pull_dict_2['selftext'].append(['not there'])\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #print(f'You have obtained {len(pull_dict_2)} posts') #real-time counter\n",
    "        temp_posts_2 = pd.DataFrame(pull_dict_2)\n",
    "        full_cons_df = pd.concat([full_cons_df, temp_posts_2])\n",
    "        utc = full_cons_df['created_utc'].astype('int64').min() #pulls the final timestamp\n",
    "        time.sleep(30)                                            #to obtain unique data\n",
    "    \n",
    "    print(f'Pull complete; you have obtained {len(full_cons_df)} total posts from r/{subreddit_2}')\n",
    "        \n",
    "        #else:\n",
    "            #print(res.status_code)                    \n",
    "#---------- combine dfs -----------------------\n",
    "    \n",
    "    full_pull_df = pd.concat([full_lib_df, full_cons_df])\n",
    "    \n",
    "    return full_pull_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    " - Successfully obtained 8000 posts; 4000 from each subreddit.\n",
    " - Below is an example pull of only 40 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pull complete; you have obtained 20 total posts from r/AskALiberal\n",
      "Pull complete; you have obtained 20 total posts from r/askaconservative\n"
     ]
    }
   ],
   "source": [
    "full_pull_df = full_pull('AskALiberal', 'askaconservative', 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Biden plans to cancel the Keystone XL pipeline...</td>\n",
       "      <td>[https://www.cbc.ca/amp/1.5877038](https://www...</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>1610945588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020 Best of r/AskALiberal Results</td>\n",
       "      <td>#Good afternoon, everyone!\\n\\n\\nThe winners an...</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>1610943721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Place your bets: will Trump be removed by forc...</td>\n",
       "      <td>We already know Trump will not attend Biden's ...</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>1610942754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Have you ever gotten conservatives to rethink ...</td>\n",
       "      <td>I’m had both positive/negative conversations f...</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>1610942080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is winning the culture war right now?</td>\n",
       "      <td>Liberals? Conservatives? China?</td>\n",
       "      <td>AskALiberal</td>\n",
       "      <td>1610939740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Biden plans to cancel the Keystone XL pipeline...   \n",
       "1                 2020 Best of r/AskALiberal Results   \n",
       "2  Place your bets: will Trump be removed by forc...   \n",
       "3  Have you ever gotten conservatives to rethink ...   \n",
       "4          Who is winning the culture war right now?   \n",
       "\n",
       "                                            selftext    subreddit created_utc  \n",
       "0  [https://www.cbc.ca/amp/1.5877038](https://www...  AskALiberal  1610945588  \n",
       "1  #Good afternoon, everyone!\\n\\n\\nThe winners an...  AskALiberal  1610943721  \n",
       "2  We already know Trump will not attend Biden's ...  AskALiberal  1610942754  \n",
       "3  I’m had both positive/negative conversations f...  AskALiberal  1610942080  \n",
       "4                    Liberals? Conservatives? China?  AskALiberal  1610939740  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pull_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8000 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        8000 non-null   object\n",
      " 1   selftext     8000 non-null   object\n",
      " 2   subreddit    8000 non-null   object\n",
      " 3   created_utc  8000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 312.5+ KB\n"
     ]
    }
   ],
   "source": [
    "full_pull_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 4)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pull_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pull_df.to_csv('full_pull_4000_each_incl_self_text.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
